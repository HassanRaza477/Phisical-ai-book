"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[8317],{3756:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"chapters/part2/chapter6","title":"Chapter 6: Vision Systems and Perception","description":"In the last chapter, we accomplished something extraordinary: we generated a perfectly labeled, randomized dataset from a simulator. This chapter is the payoff. We will take that synthetic data and use it to give our robot the gift of sight.","source":"@site/docs/chapters/part2/chapter6.md","sourceDirName":"chapters/part2","slug":"/chapters/part2/chapter6","permalink":"/Phisical-ai-book/docs/chapters/part2/chapter6","draft":false,"unlisted":false,"editUrl":"https://github.com/HassanRaza477/Phisical-ai-book.git/docs/chapters/part2/chapter6.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: NVIDIA Isaac Sim: From Simulation to Synthetic Data","permalink":"/Phisical-ai-book/docs/chapters/part2/chapter5"},"next":{"title":"Chapter 7: Vision-Language-Action (VLA) Models","permalink":"/Phisical-ai-book/docs/chapters/part2/chapter7"}}');var o=t(4848),s=t(8453);const r={},a="Chapter 6: Vision Systems and Perception",c={},l=[{value:"A Quick Review of Vision Fundamentals",id:"a-quick-review-of-vision-fundamentals",level:2},{value:"Training an Object Detection Model",id:"training-an-object-detection-model",level:2},{value:"Integrating the Model into a ROS 2 Node",id:"integrating-the-model-into-a-ros-2-node",level:2},{value:"The Payoff",id:"the-payoff",level:3}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-6-vision-systems-and-perception",children:"Chapter 6: Vision Systems and Perception"})}),"\n",(0,o.jsx)(n.p,{children:"In the last chapter, we accomplished something extraordinary: we generated a perfectly labeled, randomized dataset from a simulator. This chapter is the payoff. We will take that synthetic data and use it to give our robot the gift of sight."}),"\n",(0,o.jsx)(n.p,{children:'You will learn how to train an AI model on the data from Isaac Sim and then build a ROS 2 node that uses that model to "see" and identify objects in the world. This is the first major step in building the "intelligence" part of our Physical AI stack.'}),"\n",(0,o.jsx)(n.h2,{id:"a-quick-review-of-vision-fundamentals",children:"A Quick Review of Vision Fundamentals"}),"\n",(0,o.jsx)(n.p,{children:"Before we dive in, let's quickly cover two core concepts:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Camera Models"}),": A camera projects a 3D world onto a 2D image plane. This projection is described by a mathematical model, which includes the camera's ",(0,o.jsx)(n.strong,{children:"intrinsics"})," (focal length, optical center) and ",(0,o.jsx)(n.strong,{children:"extrinsics"})," (its position and orientation in the world). When we calibrate a camera, we are solving for these parameters."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"The ROS 2 Image Pipeline"}),": In ROS 2, images are just another type of message, typically ",(0,o.jsx)(n.code,{children:"sensor_msgs/msg/Image"}),". They are published on topics. We can use tools like ",(0,o.jsx)(n.code,{children:"rqt_image_view"})," to get a quick view of a camera feed or ",(0,o.jsx)(n.code,{children:"rviz2"})," to project the image into the 3D world."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"training-an-object-detection-model",children:"Training an Object Detection Model"}),"\n",(0,o.jsx)(n.p,{children:'Our goal is to train a model that can identify the "red cube" from the dataset we designed in Chapter 5. We\'ll use a popular, lightweight object detection architecture like YOLO (You Only Look Once).'}),"\n",(0,o.jsx)(n.p,{children:"The steps are as follows:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Set up a Training Environment"}),": This typically involves setting up a Python environment with deep learning frameworks like PyTorch or TensorFlow."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Load the Dataset"}),": You will write a script to load the images and the corresponding bounding box annotations generated by Isaac Sim's Replicator."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Choose a Model"}),": We will use a pre-trained YOLO model and fine-tune it on our specific dataset. Fine-tuning adapts a large, general-purpose model to our specific task, which is much faster than training from scratch."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Train"}),": We run the training process. The script will feed images to the model, the model will predict bounding boxes, the script will compare the predictions to our ground-truth labels, and the model will adjust its weights to improve its accuracy."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Export the Model"}),": After training, we save the model's final weights to a file (e.g., a ",(0,o.jsx)(n.code,{children:".pt"})," or ",(0,o.jsx)(n.code,{children:".onnx"})," file). This file ",(0,o.jsx)(n.em,{children:"is"})," our trained perception model."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"(Code Block: A Python snippet showing a typical training loop using PyTorch.)"})}),"\n",(0,o.jsx)(n.h2,{id:"integrating-the-model-into-a-ros-2-node",children:"Integrating the Model into a ROS 2 Node"}),"\n",(0,o.jsx)(n.p,{children:"This is where the magic happens. We will create a ROS 2 Python node that acts as the robot's perception system."}),"\n",(0,o.jsxs)(n.p,{children:["Here's the logic for our ",(0,o.jsx)(n.code,{children:"perception_node.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Illustrative example of a perception node\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, Detection2D # Custom message for results\nimport cv2\nfrom cv_bridge import CvBridge\n# Assume 'my_yolo_model' is a class that wraps our trained model\nfrom .my_yolo_model import YOLOModel \n\nclass PerceptionNode(Node):\n    def __init__(self):\n        super().__init__('perception_node')\n        self.bridge = CvBridge()\n        self.model = YOLOModel('path/to/our/trained/model.pt') # Load the model\n\n        self.subscription = self.create_subscription(\n            Image,\n            '/camera/image_raw', # Subscribe to the robot's camera feed\n            self.image_callback,\n            10)\n        \n        self.publisher_ = self.create_publisher(Detection2DArray, '/perception/detections', 10)\n        self.get_logger().info('Perception Node has started.')\n\n    def image_callback(self, msg):\n        # 1. Convert the ROS Image message to an OpenCV image\n        cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n        # 2. Run inference\n        results = self.model.predict(cv_image) # This is where the AI does its work\n\n        # 3. Process results and publish them\n        detection_array = Detection2DArray()\n        detection_array.header = msg.header\n\n        for res in results:\n            # Create a Detection2D message for each detected object\n            detection = Detection2D()\n            # ... fill in bounding box, class name, score ...\n            detection_array.detections.append(detection)\n\n        self.publisher_.publish(detection_array)\n        self.get_logger().info(f'Detected {len(results)} objects.')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PerceptionNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"the-payoff",children:"The Payoff"}),"\n",(0,o.jsx)(n.p,{children:"Now, when you run your simulation and this perception node:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"The simulated camera in Isaac Sim publishes raw images."}),"\n",(0,o.jsxs)(n.li,{children:["Our ",(0,o.jsx)(n.code,{children:"perception_node"})," receives the images."]}),"\n",(0,o.jsx)(n.li,{children:"It uses the AI model we trained on synthetic data to find the red cube in the image."}),"\n",(0,o.jsxs)(n.li,{children:["It publishes the location of the red cube as a ",(0,o.jsx)(n.code,{children:"Detection2DArray"})," message."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Another ROS 2 node can now subscribe to ",(0,o.jsx)(n.code,{children:"/perception/detections"})," to know where the cube is. We have successfully closed the loop: from synthetic data generation to a functional, AI-powered perception system running in ROS 2."]}),"\n",(0,o.jsx)(n.p,{children:"This is a monumental step. Our robot is no longer blind. In the next chapter, we'll learn how to connect this newfound sense of sight to language and action."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var i=t(6540);const o={},s=i.createContext(o);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);